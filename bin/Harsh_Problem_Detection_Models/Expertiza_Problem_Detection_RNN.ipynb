{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Expertiza_Problem_Detection_RNN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "1sCKyAJwaQ8U",
        "colab_type": "code",
        "outputId": "7cf54b95-229f-4e25-c2f3-acbbe9e473d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g3w0X29SazeM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp gdrive/My\\ Drive/AI-In-Peer-Assessment/problems_expertiza_merged_gabe.csv ."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j0APyYiadyoh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# To store data\n",
        "import pandas as pd\n",
        "\n",
        "# To use regular expressions\n",
        "import re\n",
        "\n",
        "#To load and save data\n",
        "import pickle\n",
        "\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "12b4F6H_d7h8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "reviews = pd.read_csv(\"problems_expertiza_merged_gabe.csv\") "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0YwczJUmdzvx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = reviews.filter([\"REVIEW\",\"TAG\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zq9fP3BCeJkl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = data.loc[data.REVIEW.apply(lambda x: not isinstance(x, (float, int)))]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cTZMJKe9d-D8",
        "colab_type": "code",
        "outputId": "eaa6066a-2ddc-4b58-9150-b0cfbb61182c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        }
      },
      "source": [
        "data['TAG'].value_counts()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    5585\n",
              "0    5585\n",
              "Name: TAG, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LKZ2PAonePLa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_set, test_set = train_test_split(data, test_size=0.05, random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ngJUjQBLeTfz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_reviews = list(train_set[\"REVIEW\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NxHR2M_neV79",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_labels = list(train_set[\"TAG\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u6uKOYZuebCz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(len(train_reviews)):\n",
        "  train_reviews[i] = re.sub('\\d',' ',train_reviews[i]) # Replacing digits by space\n",
        "  train_reviews[i] = re.sub(r'\\s+[a-z][\\s$]', ' ',train_reviews[i]) # Removing single characters and spaces alongside\n",
        "  train_reviews[i] = re.sub(r'\\s+', ' ',train_reviews[i]) # Replacing more than one space with a single space"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jv5Y3Z_5edV9",
        "colab_type": "code",
        "outputId": "e86b63a7-00d4-4775-849f-30d99aa70453",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        }
      },
      "source": [
        "for i in range(len(train_reviews)):\n",
        "    if 'www.' in train_reviews[i] or 'http:' in train_reviews[i] or 'https:' in train_reviews[i] or '.com' in train_reviews[i]:\n",
        "        train_reviews[i] = re.sub(r\"([^ ]+(?<=\\.[a-z]{3}))\", \"<url>\", train_reviews[i])\n",
        "        \n",
        "        \n",
        "train_reviews[1:5]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Yes. The team had bulleted list of tasked items to implement in the Proposed Imports Changes section.',\n",
              " 'Yes the design doc incorporates all the functionality mentioned in the above link.',\n",
              " 'They have test on most controllers and the user model',\n",
              " 'Yes, the team has indeed added test case and made test plan for both the issues that they have resolved.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rkMhg6eaegi7",
        "colab_type": "code",
        "outputId": "95ad610b-0ff9-45d4-919e-68be238f2609",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "# Use Tokenizer to remove punctuations and non-word characters and tokenize the text\n",
        "import os\n",
        "os.environ['KERAS_BACKEND']='tensorflow' # Or TenserFlow\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import *\n",
        "from keras.models import Model\n",
        "from keras.callbacks import ModelCheckpoint"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vDfyfBe5ehsg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_SEQUENCE_LENGTH = 80\n",
        "MAX_NB_WORDS = 80 # This specifies how many top tokens in each review to be stored. Wrongly interpreted as total number of words(token) together in whole dataset\n",
        "EMBEDDING_DIM = 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43oqtlxEelWo",
        "colab_type": "code",
        "outputId": "bf6018b1-21ae-4675-d786-1076b044b364",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "# Takes 5 minutes to run on entire training dataset\n",
        "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
        "tokenizer.fit_on_texts(train_reviews)\n",
        "train_sequences = tokenizer.texts_to_sequences(train_reviews)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "print('Number of Unique Tokens',len(word_index)) # Total 996497 unique words "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of Unique Tokens 8577\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XbDeOsHIepS8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Padding\n",
        "train_sequences_padded = pad_sequences(train_sequences, maxlen=MAX_SEQUENCE_LENGTH)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vek9L-PzesMP",
        "colab_type": "code",
        "outputId": "8517ada7-eaf0-44bb-da43-96588cc967ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        }
      },
      "source": [
        "!cp gdrive/My\\ Drive/glove100d.zip .\n",
        "!unzip glove100d.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  glove100d.zip\n",
            "replace glove.6B.100d.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: glove.6B.100d.txt       \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Dl1xbaXetAs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embeddings_index = {}\n",
        "for i, line in enumerate(open('glove.6B.100d.txt')):\n",
        "  values = line.split() # 0 th index will be the word and rest will the embedding vector (size 100 as we have used Glove.6B.100D embedding file) \n",
        "  embeddings_index[values[0]] = np.asarray(values[1:], dtype='float32')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kDSxmSAKewsr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create token(words in word index)-embedding mapping\n",
        "embedding_matrix = np.zeros((len(word_index) + 1, 100)) # 100 since embedding_dimesion is 100, +1 because index 0 is reserved in word_index\n",
        "for word, i in word_index.items():\n",
        "  embedding_vector = embeddings_index.get(word)\n",
        "  if embedding_vector is not None:\n",
        "    embedding_matrix[i] = embedding_vector\n",
        "# We can initialize random vector and assign for words which are not present in embeddings.Other option is keep trainable=true in embedding layer of the NN model.\n",
        "# We choose 2nd option"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SIARo4KQe5LB",
        "colab_type": "code",
        "outputId": "55d2172a-dcf0-4537-c033-7f3c8991feac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "nonzero_elements = np.count_nonzero(np.count_nonzero(embedding_matrix, axis=1))\n",
        "nonzero_elements / len(word_index)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6510434883992072"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MgXGAYP3fF5p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.utils import shuffle\n",
        "x_train, y_train = shuffle(train_sequences_padded, train_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HSSxJVQ5fJwc",
        "colab_type": "code",
        "outputId": "a7f765b8-e1b0-46a1-8bce-ccbbb57e2728",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "x_train = np.array(x_train[:])\n",
        "train_labels = [[1,0] if x == 1 else [0,1] for x in y_train[:]] \n",
        "y_train = np.array(train_labels[:])\n",
        "len(x_train),len(y_train)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10611, 10611)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9u77y7NvfWJm",
        "colab_type": "code",
        "outputId": "b1c14108-ef9e-42b3-9801-a8751d509e58",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "embedding_layer = Embedding(len(word_index) + 1,\n",
        "                            EMBEDDING_DIM,weights=[embedding_matrix],\n",
        "                            input_length=MAX_SEQUENCE_LENGTH,trainable=True)\n",
        "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
        "embedded_sequences = embedding_layer(sequence_input)\n",
        "\n",
        "net = Dropout(0.3)(embedded_sequences)\n",
        "net = Bidirectional(LSTM(200,recurrent_dropout=0.4))(net)\n",
        "net = Dropout(0.3)(net)\n",
        "output = Dense(2, activation = 'softmax')(net)\n",
        "model = Model(inputs = sequence_input, outputs = output)\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
        "model.summary()\n",
        "\n",
        "# Keeping a checkpoint to store only the model which gives best output validation accuracy\n",
        "chkpt=ModelCheckpoint('expertiza_rnn_model.h5',monitor='val_acc',verbose=1,save_best_only=True)\n",
        "model_history = model.fit(x_train, y_train, batch_size=256, epochs=10, validation_split=0.2,callbacks=[chkpt])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         (None, 80)                0         \n",
            "_________________________________________________________________\n",
            "embedding_1 (Embedding)      (None, 80, 100)           857800    \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 80, 100)           0         \n",
            "_________________________________________________________________\n",
            "bidirectional_1 (Bidirection (None, 400)               481600    \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 400)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 2)                 802       \n",
            "=================================================================\n",
            "Total params: 1,340,202\n",
            "Trainable params: 1,340,202\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "Train on 8488 samples, validate on 2123 samples\n",
            "Epoch 1/10\n",
            "8488/8488 [==============================] - 82s 10ms/step - loss: 0.5900 - acc: 0.6786 - val_loss: 0.5140 - val_acc: 0.7489\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.74894, saving model to expertiza_rnn_model.h5\n",
            "Epoch 2/10\n",
            "8488/8488 [==============================] - 80s 9ms/step - loss: 0.5006 - acc: 0.7612 - val_loss: 0.4652 - val_acc: 0.7885\n",
            "\n",
            "Epoch 00002: val_acc improved from 0.74894 to 0.78851, saving model to expertiza_rnn_model.h5\n",
            "Epoch 3/10\n",
            "8488/8488 [==============================] - 79s 9ms/step - loss: 0.4714 - acc: 0.7895 - val_loss: 0.4605 - val_acc: 0.8022\n",
            "\n",
            "Epoch 00003: val_acc improved from 0.78851 to 0.80217, saving model to expertiza_rnn_model.h5\n",
            "Epoch 4/10\n",
            "8488/8488 [==============================] - 79s 9ms/step - loss: 0.4444 - acc: 0.8037 - val_loss: 0.4324 - val_acc: 0.8069\n",
            "\n",
            "Epoch 00004: val_acc improved from 0.80217 to 0.80688, saving model to expertiza_rnn_model.h5\n",
            "Epoch 5/10\n",
            "8488/8488 [==============================] - 80s 9ms/step - loss: 0.4351 - acc: 0.8103 - val_loss: 0.4287 - val_acc: 0.8088\n",
            "\n",
            "Epoch 00005: val_acc improved from 0.80688 to 0.80876, saving model to expertiza_rnn_model.h5\n",
            "Epoch 6/10\n",
            "8488/8488 [==============================] - 82s 10ms/step - loss: 0.4223 - acc: 0.8133 - val_loss: 0.4215 - val_acc: 0.8130\n",
            "\n",
            "Epoch 00006: val_acc improved from 0.80876 to 0.81300, saving model to expertiza_rnn_model.h5\n",
            "Epoch 7/10\n",
            "8488/8488 [==============================] - 81s 10ms/step - loss: 0.4191 - acc: 0.8179 - val_loss: 0.4166 - val_acc: 0.8168\n",
            "\n",
            "Epoch 00007: val_acc improved from 0.81300 to 0.81677, saving model to expertiza_rnn_model.h5\n",
            "Epoch 8/10\n",
            "8488/8488 [==============================] - 81s 10ms/step - loss: 0.4153 - acc: 0.8212 - val_loss: 0.4175 - val_acc: 0.8172\n",
            "\n",
            "Epoch 00008: val_acc improved from 0.81677 to 0.81724, saving model to expertiza_rnn_model.h5\n",
            "Epoch 9/10\n",
            "8488/8488 [==============================] - 88s 10ms/step - loss: 0.4136 - acc: 0.8221 - val_loss: 0.4223 - val_acc: 0.8154\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.81724\n",
            "Epoch 10/10\n",
            "8488/8488 [==============================] - 98s 12ms/step - loss: 0.4111 - acc: 0.8246 - val_loss: 0.4153 - val_acc: 0.8220\n",
            "\n",
            "Epoch 00010: val_acc improved from 0.81724 to 0.82195, saving model to expertiza_rnn_model.h5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BeiCnoJ9ZrUa",
        "colab_type": "code",
        "outputId": "2e14e44c-3df3-459e-c23d-ecb032d07f41",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "embedding_layer = Embedding(len(word_index) + 1,\n",
        "                            EMBEDDING_DIM,\n",
        "                            input_length=MAX_SEQUENCE_LENGTH,trainable=True)\n",
        "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
        "embedded_sequences_1 = embedding_layer(sequence_input)\n",
        "\n",
        "net1 = Dropout(0.3)(embedded_sequences_1)\n",
        "net1 = Conv1D(50, 3, padding='same', activation='relu')(net1)\n",
        "net1 = AveragePooling1D(pool_size=4)(net1)\n",
        "net1 = LSTM(100, recurrent_dropout=0.3)(net1)\n",
        "net1 = Dropout(0.2)(net1)\n",
        "output1 = Dense(2, activation='softmax')(net1)\n",
        "\n",
        "model5 = Model(inputs = sequence_input, outputs = output1)\n",
        "model5.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
        "model5.summary()\n",
        "\n",
        "# Keeping a checkpoint to store only the model which gives best output validation accuracy\n",
        "chkpt=ModelCheckpoint('expertiza_cnn_rnn_model.h5',monitor='val_acc',verbose=1,save_best_only=True)\n",
        "model_history1 = model5.fit(x_train, y_train, batch_size=100, epochs=10, validation_split=0.1,callbacks=[chkpt])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4271: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
            "\n",
            "Model: \"model_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         (None, 80)                0         \n",
            "_________________________________________________________________\n",
            "embedding_2 (Embedding)      (None, 80, 100)           857800    \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 80, 100)           0         \n",
            "_________________________________________________________________\n",
            "conv1d_1 (Conv1D)            (None, 80, 50)            15050     \n",
            "_________________________________________________________________\n",
            "average_pooling1d_1 (Average (None, 20, 50)            0         \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (None, 100)               60400     \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 2)                 202       \n",
            "=================================================================\n",
            "Total params: 933,452\n",
            "Trainable params: 933,452\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 9549 samples, validate on 1062 samples\n",
            "Epoch 1/10\n",
            "9549/9549 [==============================] - 15s 2ms/step - loss: 0.5472 - acc: 0.7163 - val_loss: 0.4348 - val_acc: 0.8173\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.81733, saving model to expertiza_cnn_rnn_model.h5\n",
            "Epoch 2/10\n",
            "9549/9549 [==============================] - 17s 2ms/step - loss: 0.4197 - acc: 0.8153 - val_loss: 0.4104 - val_acc: 0.8230\n",
            "\n",
            "Epoch 00002: val_acc improved from 0.81733 to 0.82298, saving model to expertiza_cnn_rnn_model.h5\n",
            "Epoch 3/10\n",
            "9549/9549 [==============================] - 11s 1ms/step - loss: 0.4061 - acc: 0.8253 - val_loss: 0.4071 - val_acc: 0.8343\n",
            "\n",
            "Epoch 00003: val_acc improved from 0.82298 to 0.83427, saving model to expertiza_cnn_rnn_model.h5\n",
            "Epoch 4/10\n",
            "9549/9549 [==============================] - 14s 1ms/step - loss: 0.4053 - acc: 0.8260 - val_loss: 0.4074 - val_acc: 0.8164\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.83427\n",
            "Epoch 5/10\n",
            "9549/9549 [==============================] - 15s 2ms/step - loss: 0.3969 - acc: 0.8305 - val_loss: 0.3996 - val_acc: 0.8277\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.83427\n",
            "Epoch 6/10\n",
            "9549/9549 [==============================] - 15s 2ms/step - loss: 0.3971 - acc: 0.8337 - val_loss: 0.4025 - val_acc: 0.8230\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.83427\n",
            "Epoch 7/10\n",
            "9549/9549 [==============================] - 17s 2ms/step - loss: 0.3910 - acc: 0.8339 - val_loss: 0.4025 - val_acc: 0.8211\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.83427\n",
            "Epoch 8/10\n",
            "9549/9549 [==============================] - 18s 2ms/step - loss: 0.3901 - acc: 0.8350 - val_loss: 0.4013 - val_acc: 0.8239\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.83427\n",
            "Epoch 9/10\n",
            "9549/9549 [==============================] - 16s 2ms/step - loss: 0.3897 - acc: 0.8371 - val_loss: 0.3975 - val_acc: 0.8267\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.83427\n",
            "Epoch 10/10\n",
            "9549/9549 [==============================] - 15s 2ms/step - loss: 0.3863 - acc: 0.8385 - val_loss: 0.4014 - val_acc: 0.8249\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.83427\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oMZtHwiejmc_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Saving the model so that it can be loaded easily again\n",
        "model.save_weights('expertiza_rnn_model_weights.h5')\n",
        "\n",
        "# Save the model architecture\n",
        "with open('expertiza_rnn_model_architecture.json', 'w') as f:\n",
        "    f.write(model.to_json())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v704nFqhj9Jw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Persisting model weights\n",
        "!cp expertiza_rnn_model_weights.h5 gdrive/My\\ Drive/AI-In-Peer-Assessment/model/\n",
        "# Persisting model architecture\n",
        "!cp expertiza_rnn_model_architecture.json gdrive/My\\ Drive/AI-In-Peer-Assessment/model/"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}